import requests
import time
import json


import os

def _get_notion_token():
    # Try multiple paths to find notion_key.txt
    current_dir = os.path.dirname(os.path.abspath(__file__))
    paths = [
        os.path.join(current_dir, 'notion_key.txt'),
        os.path.join(current_dir, '..', 'core', 'notion_key.txt'),
        os.path.join(current_dir, 'core', 'notion_key.txt'),
        os.path.join(os.getcwd(), 'notion_automation', 'core', 'notion_key.txt')
    ]
    for p in paths:
        if os.path.exists(p):
            with open(p, 'r', encoding='utf-8') as f:
                token = f.read().strip()
                if token: return token
    return os.getenv("NOTION_TOKEN", "YOUR_NOTION_TOKEN_HERE")

NOTION_TOKEN = _get_notion_token()
PAGE_ID = '2f0eacc8-175a-805c-85b2-dca59899d3d8'
HEADERS = {
    'Authorization': f'Bearer {NOTION_TOKEN}',
    'Content-Type': 'application/json',
    'Notion-Version': '2022-06-28'
}

def update_notion_verbatim():
    # 1. ì›ë³¸ íŒŒì¼ì—ì„œ ì½”ë“œì™€ ì£¼ì„ ì½ê¸°
    with open('gitp/BFS/2573ëíŒì™• bfs ë™ì‹œíƒìƒ‰ì‹œ ì‹œê°„ì´ˆê³¼ ë”°ë¼ì„œ ë³€ë™ê°’ ë¦¬ìŠ¤íŠ¸ ì €ì¥.py', 'r', encoding='utf-8') as f:
        verbatim_code = f.read()

    # 2. ì „ëµ ë¶„ì„ í…ìŠ¤íŠ¸ êµ¬ì„±
    strategy_analysis = """
### ğŸ” [IM ì´ˆì›”] 3ë‹¨ê³„ í•µì‹¬ ì „ëµ ë¶„ì„ (ì‚¬ìš©ì ì›ë³¸ ê°€ì´ë“œ)

**1ë‹¨ê³„: "ë²”ì¸ì€ ì´ ì•ˆì— ìˆì–´!" (ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ í™œìš©)**
*   í˜•ì‚¬ê°€ ë²”ì¸ì„ ì¡ìœ¼ëŸ¬ ê°ˆ ë•Œ, ë„ì‹œ ì „ì²´ 9ë§Œ ê°€êµ¬ë¥¼ ì§‘ì§‘ë§ˆë‹¤ ë°©ë¬¸(ì´ì¤‘ forë¬¸)í•˜ë©´ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ê² ì£ ?
*   ëŒ€ì‹  **"ìš©ì˜ì ëª…ë‹¨(ice_list)"**ë§Œ ë“¤ê³  ê·¸ ì§‘ë“¤ë§Œ ì°¾ì•„ê°€ëŠ” ê²Œ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.
*   **ê²°ê³¼**: ë§¤ë…„ ë£¨í”„ë¥¼ ëŒ ë•Œë§ˆë‹¤ í™•ì¸í•˜ëŠ” ì¹¸ì´ 90,000ê°œì—ì„œ ìˆ˜ë°± ê°œë¡œ í™• ì¤„ì–´ë“­ë‹ˆë‹¤.

**2ë‹¨ê³„: "ìŠ¤ëƒ…ìƒ· ì°ê¸°" (ì˜ˆì•½ ì‹œìŠ¤í…œ)**
*   ë¹™ì‚° í•˜ë‚˜ê°€ ë…¹ì•„ 0ì´ ë˜ëŠ” ìˆœê°„ ì˜† ì¹¸ì˜ ê²°ê³¼ì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ë¬¸ì œëŠ” "ë™ì‹œì—" ë…¹ëŠ” ê²ƒì„ ì›í•˜ì£ .
*   "ì§€ê¸ˆ ë°”ë¡œ ì§€ë„ë¥¼ ê³ ì¹˜ë©´ ë‹¤ìŒ ì¹¸ ê³„ì‚°ì´ ê¼¬ì¸ë‹¤. ê·¸ëŸ¬ë‹ˆ 'ëˆ„ê°€ ì–¼ë§ˆë‚˜ ë…¹ì„ì§€' ë©”ëª¨ì§€(melt_list)ì— ì¼ë‹¨ ì ì–´ë§Œ ë‘ì. ì¡°ì‚¬ê°€ ë‹¤ ëë‚˜ë©´ ê·¸ë•Œ í•œêº¼ë²ˆì— ì§€ë„ë¥¼ ê³ ì¹˜ì(Batch Update)."
*   **ê²°ê³¼**: ì—°ì‡„ ë°˜ì‘ ì˜¤ë¥˜ë¥¼ ë§‰ê³  ë°ì´í„°ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

**3ë‹¨ê³„: "ë‹¤ì´ì–´íŠ¸ ì‹œí‚¤ê¸°" (ë¦¬ìŠ¤íŠ¸ ê°±ì‹ )**
*   ëª…ë‹¨ì— ìˆëŠ” ìš©ì˜ìê°€ ì´ë¯¸ ê°ì˜¥ì— ê°”ê±°ë‚˜ ì‚¬ë¼ì¡Œë‹¤ë©´, ë‚´ë…„ ëª…ë‹¨ì—ì„œëŠ” ë¹¼ì•¼ í•©ë‹ˆë‹¤.
*   ì˜¬í•´ ë…¹ì•„ì„œ 0ì´ ëœ ì• ë“¤ì€ ë‚´ë…„ì—” ê²€ì‚¬í•  í•„ìš”ê°€ ì—†ì–ì•„? ë‚´ë…„ìš© ìƒˆ ëª…ë‹¨(next_ice_list)ì„ ë§Œë“¤ì–´ì„œ ì‚´ì•„ë‚¨ì€ ì• ë“¤ë§Œ ì˜®ê²¨ ë‹´ì.
*   **ê²°ê³¼**: ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ê²€ì‚¬í•  ëŒ€ìƒì´ ì¤„ì–´ë“¤ì–´ ì†ë„ê°€ ì ì  ë” ë¹¨ë¼ì§‘ë‹ˆë‹¤.
"""

    blocks = [
        {'type': 'divider', 'divider': {}},
        {'type': 'heading_1', 'heading_1': {'rich_text': [{'type': 'text', 'text': {'content': 'ğŸ“ [Samsung A] ë¹™ì‚° - BFS ê¸°ë°˜ ë™ì‹œ ì‹œë®¬ë ˆì´ì…˜ (ì›ë³¸ ì£¼ì„ 100% ë³´ì „)'}}]}},
        {'type': 'quote', 'quote': {'rich_text': [{'type': 'text', 'text': {'content': 'ì›ë³¸ íŒŒì¼ì˜ ì½”ë“œ ë‚´ ì£¼ì„("ìª½ì§€", "ë°”ë‹¤ì˜ ê°œìˆ˜ë§Œí¼", "0ë¯¸ë§Œ ë°©ì–´" ë“±)ì„ í•œ ê¸€ìë„ ë¹ ì§ì—†ì´ ì™„ë²½í•˜ê²Œ ë³µì œí–ˆìŠµë‹ˆë‹¤.'}}]}},
        {'type': 'heading_2', 'heading_2': {'rich_text': [{'type': 'text', 'text': {'content': 'ğŸ” í•µì‹¬ ì „ëµ ë° ìƒê°ì˜ íë¦„'}}]}},
        {'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': strategy_analysis.strip()}}]}},
        {'type': 'heading_2', 'heading_2': {'rich_text': [{'type': 'text', 'text': {'content': 'ğŸ’» Python ì •ë‹µ ì½”ë“œ (ì›ë³¸ ì£¼ì„ í¬í•¨)'}}]}},
        {'type': 'code', 'code': {'language': 'python', 'rich_text': [{'type': 'text', 'text': {'content': verbatim_code.strip()}}]}},
        {'type': 'callout', 'callout': {'icon': {'type': 'emoji', 'emoji': 'ğŸ’¡'}, 'rich_text': [{'type': 'text', 'text': {'content': 'í•™ìƒ ê°€ì´ë“œ: ì›ë³¸ ì½”ë“œì˜ ì£¼ì„ì´ì•¼ë§ë¡œ ì‹¤ì „ì—ì„œ ë– ì˜¬ë ¤ì•¼ í•  "ìƒê°ì˜ ì§€ë„"ì…ë‹ˆë‹¤. 9ë§Œ ì¹¸ ëŒ€ì‹  ëª…ë‹¨ë§Œ í™•ì¸í•˜ëŠ” ìµœì í™” ê¸°ë²•ì„ ì™„ë²½íˆ ìˆ™ì§€í•˜ì„¸ìš”.'}}]}}
    ]

    # 3. ê¸°ì¡´ ë¸”ë¡ ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
    url = f'https://api.notion.com/v1/blocks/{PAGE_ID}/children'
    res = requests.get(url, headers=HEADERS)
    all_blocks = res.json().get('results', [])
    
    target_start_index = -1
    for i, b in enumerate(all_blocks):
        if b['type'] == 'heading_1' and 'ë¹™ì‚°' in b['heading_1']['rich_text'][0]['plain_text']:
            target_start_index = i
            break
            
    if target_start_index != -1:
        print(f"Cleaning up blocks from index {target_start_index}...")
        for b in all_blocks[target_start_index:]:
            requests.delete(f'https://api.notion.com/v1/blocks/{b["id"]}', headers=HEADERS)
            time.sleep(0.1)

    # 4. 100% ë³´ì¡´ëœ ì½˜í…ì¸  ì¶”ê°€
    for i in range(0, len(blocks), 5):
        chunk = blocks[i:i+5]
        requests.patch(url, headers=HEADERS, json={'children': chunk})
        time.sleep(1)
    print("Success: Updated Notion with 100% verbatim comments from the original file.")

if __name__ == '__main__':
    update_notion_verbatim()
